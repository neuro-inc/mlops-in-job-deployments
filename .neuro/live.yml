kind: live
title: mlops-mlflow-model-deploy

volumes:
  triton_model_repo:
    remote: storage:${{ project.id }}/triton_model_repo
    mount: /tmp/triton_model_repo
  data:
    remote: "storage:"
    mount: /project/data
  code:
    remote: "storage:"
    mount: /project/modules
  config:
    remote: "storage:"
    mount: /project/config
  notebooks:
    remote: "storage:${{ project.id }}/examples"
    mount: /project/notebooks
    local: examples
  results:
    remote: "storage:"
    mount: /project/results
  project:
    remote: "storage:"
    mount: /project
images:
  app:
    ref: $[[ fmt("image:{}:{}", project.id, git.sha) ]]
    dockerfile: $[[ flow.workspace ]]/Dockerfile
    context: $[[ flow.workspace ]]/


jobs:
  app:
    image: $[[ images.app.ref ]]
    browse: true
    pass_config: true
    http_port: 8501
    volumes:
      - ${{ volumes.triton_model_repo.ref_rw }}
    env:
      MLFLOW_TRACKING_URI: ${{ params.mlflow_uri }}
      TRITON_MODEL_REPO: ${{ volumes.triton_model_repo.mount }}
      TRITON_MODEL_REPO_STORAGE: ${{ volumes.triton_model_repo.remote }}
    params:
      mlflow_uri:
        default:
        descr: |
          MLFlow server URI running on the platform to connect to.
          The server should be running with `--serve-artifacts` option enabled

  webdav:
    # used for local development.
    # run this job and perform
    # mount_webdav <job-uri> /tmp/triton_model_repo
    action: gh:neuro-actions/webdav_server@v1.0.0
    args:
      volume_remote: ${{ volumes.triton_model_repo.remote }}
      job_lifespan: 30d

  code_examples:
    action: gh:neuro-actions/jupyter@master
    args:
      volumes_data_remote: ${{ volumes.data.remote }}
      volumes_code_remote: ${{ volumes.code.remote }}
      volumes_config_remote: ${{ volumes.config.remote }}
      volumes_notebooks_remote: ${{ volumes.notebooks.remote }}
      volumes_results_remote: ${{ volumes.results.remote }}
