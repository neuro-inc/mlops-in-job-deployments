kind: live
title: mlops-mlflow-model-deploy

volumes:
  triton_model_repo:
    remote: storage:${{ project.id }}/triton_model_repo
    mount: /tmp/triton_model_repo

images:
  app:
    ref: $[[ fmt("image:{}:{}", project.id, git.sha) ]]
    dockerfile: $[[ flow.workspace ]]/Dockerfile
    context: $[[ flow.workspace ]]/


jobs:
  app:
    image: $[[ images.app.ref ]]
    browse: true
    pass_config: true
    http_port: 8501
    volumes:
      - ${{ volumes.triton_model_repo.ref_rw }}
    env:
      MLFLOW_TRACKING_URI: ${{ params.mlflow_uri }}
      TRITON_MODEL_REPO: ${{ volumes.triton_model_repo.mount }}
      TRITON_MODEL_REPO_STORAGE: ${{ volumes.triton_model_repo.remote }}
    params:
      mlflow_uri:
        default:
        descr: |
          MLFlow server URI running on the platform to connect to.
          The server should be running with `--serve-artifacts` option enabled

  webdav:
    # used for local development.
    # run this job and perform
    # mount_webdav <job-uri> /tmp/triton_model_repo
    action: gh:neuro-actions/webdav_server@v1.0.0
    args:
      volume_remote: ${{ volumes.triton_model_repo.remote }}
      job_lifespan: 30d
